{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LegacyHalos Sample Selection\n",
    "\n",
    "The goal of this notebook is to select a sample of central and satellite galaxies from redMaPPer but with (deeper) Legacy Survey photometry from DR9.\n",
    "\n",
    "The input (row-matched) catalogs for the centrals are:\n",
    "* \\$REDMAPPER_DIR/v6.3.1/dr8_run_redmapper_v6.3.1_lgt5_catalog.fit  \n",
    "* \\$REDMAPPER_DIR/v6.3.1/redmapper-v6.3.1-lgt5-centrals-sdssWISEphot-dr14.fits (generated by [redmapper-sdssWISEphot.ipynb](https://github.com/moustakas/legacyhalos/tree/master/doc/redmapper-sdssWISEphot.ipynb))  \n",
    "* \\$REDMAPPER_DIR/v6.3.1/legacysurvey-dr9-north-centrals-v6.3.1.fits (generated by [legacyhalos-match-redmapper](https://github.com/moustakas/legacyhalos/tree/master/bin/legacyhalos/legacyhalos-match-redmapper))  \n",
    "* \\$REDMAPPER_DIR/v6.3.1/legacysurvey-dr9-south-centrals-v6.3.1.fits (generated by [legacyhalos-match-redmapper](https://github.com/moustakas/legacyhalos/tree/master/bin/legacyhalos/legacyhalos-match-redmapper)) \n",
    "\n",
    "\n",
    "and for the satellites:\n",
    "* \\$REDMAPPER_DIR/v6.3.1/dr8_run_redmapper_v6.3.1_lgt5_catalog_members.fit\n",
    "* \\$REDMAPPER_DIR/v6.3.1/redmapper-v6.3.1-lgt5-members-sdssWISEphot-dr14.fits (generated by [redmapper-sdssWISEphot.ipynb](https://github.com/moustakas/legacyhalos/tree/master/doc/redmapper-sdssWISEphot.ipynb))  \n",
    "* \\$REDMAPPER_DIR/v6.3.1/legacysurvey-dr9-north-members-v6.3.1.fits (generated by [legacyhalos-match-redmapper](https://github.com/moustakas/legacyhalos/tree/master/bin/legacyhalos/legacyhalos-match-redmapper))\n",
    "* \\$REDMAPPER_DIR/v6.3.1/legacysurvey-dr9-south-members-v6.3.1.fits (generated by [legacyhalos-match-redmapper](https://github.com/moustakas/legacyhalos/tree/master/bin/legacyhalos/legacyhalos-match-redmapper))\n",
    "\n",
    "And the resulting output catalogs are the files:\n",
    "\n",
    "* \\$LEGACYHALOS_DIR/sample/legacyhalos-centrals-dr9.fits\n",
    "* \\$LEGACYHALOS_DIR/sample/legacyhalos-candidate-centrals-dr9.fits\n",
    "\n",
    "In addition, we create jackknife subsamples of the data and write them out in the file:\n",
    "\n",
    "* \\$LEGACYHALOS_DIR/sample/legacyhalos-jackknife-dr9.fits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib created a temporary config/cache directory at /tmp/matplotlib-dheun5q9 because the default path (/homedir/.config/matplotlib) is not a writable directory; it is highly recommended to set the MPLCONFIGDIR environment variable to a writable directory, in particular to speed up the import of Matplotlib and to better support multiprocessing.\n"
     ]
    }
   ],
   "source": [
    "import os, warnings, pdb, time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import healpy as hp\n",
    "import fitsio\n",
    "from astropy.io import fits\n",
    "from astropy.table import Table, Column, vstack, hstack\n",
    "import astropy.units as u\n",
    "from astropy.coordinates import SkyCoord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import legacyhalos.io\n",
    "from legacyhalos.misc import radec2pix, pix2radec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#index = np.arange(50000)\n",
    "index = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify the LegacyHalos path and output files names.\n",
    "\n",
    "In production, these environment variables are set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: LEGACYHALOS_DIR=/global/cfs/cdirs/desi/users/ioannis/legacyhalos\n",
      "env: LEGACYHALOS_DATA_DIR=/global/cscratch1/sd/ioannis/legacyhalos-data\n",
      "env: LEGACYHALOS_HTML_DIR=/global/cfs/cdirs/cosmo/www/temp/ioannis/legacyhalos-html\n",
      "env: REDMAPPER_DIR=/global/cfs/cdirs/desi/users/ioannis/redmapper\n"
     ]
    }
   ],
   "source": [
    "%set_env LEGACYHALOS_DIR=/global/cfs/cdirs/desi/users/ioannis/legacyhalos\n",
    "%set_env LEGACYHALOS_DATA_DIR=/global/cscratch1/sd/ioannis/legacyhalos-data\n",
    "%set_env LEGACYHALOS_HTML_DIR=/global/cfs/cdirs/cosmo/www/temp/ioannis/legacyhalos-html\n",
    "%set_env REDMAPPER_DIR=/global/cfs/cdirs/desi/users/ioannis/redmapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Required ${LEGACYHALOS_DATA_DIR environment variable not set.\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-9c1a13328788>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mlegacyhalos_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlegacyhalos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacyhalos_data_dir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlegacyhalos_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmakedirs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlegacyhalos_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/src/legacyhalos/py/legacyhalos/io.py\u001b[0m in \u001b[0;36mlegacyhalos_data_dir\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'LEGACYHALOS_DATA_DIR'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Required ${LEGACYHALOS_DATA_DIR environment variable not set.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mEnvironmentError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0mldir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetenv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'LEGACYHALOS_DATA_DIR'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mldir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOSError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "legacyhalos_dir = os.getenv()\n",
    "if not os.path.exists(legacyhalos_dir):\n",
    "    os.makedirs(legacyhalos_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsdr, sdssdr, rmversion = 'dr9', 'dr14', 'v6.3.1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cenfile = os.path.join( legacyhalos.io.sample_dir(), 'legacyhalos-centrals-{}.fits'.format(lsdr) )\n",
    "candcenfile = os.path.join( legacyhalos.io.sample_dir(), 'legacyhalos-candidate-centrals-{}.fits'.format(lsdr) )\n",
    "jackfile = os.path.join( legacyhalos.io.sample_dir(), 'legacyhalos-jackknife-{}.fits'.format(lsdr) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the matched Legacy Survey and redMaPPer catalogs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_legacysurvey(rmversion='v6.3.1', index=None, satellites=False, satid=None):\n",
    "    \"\"\"Read the matched Legacy Survey catalogs.\n",
    "    \n",
    "    Note that non-matching entries are populated with zeros / False.\n",
    "    \n",
    "    \"\"\"\n",
    "    if satellites:\n",
    "        galtype = 'members'\n",
    "    else:\n",
    "        galtype = 'centrals'\n",
    "       \n",
    "    cols = ['RELEASE', 'BRICKID', 'BRICKNAME', 'OBJID', 'TYPE', 'RA', 'DEC',\n",
    "            #'RA_IVAR', 'DEC_IVAR', 'DCHISQ', \n",
    "            'EBV', 'MASKBITS',\n",
    "            #'FLUX_U', 'FLUX_I', 'FLUX_Y', \n",
    "            'FLUX_G', 'FLUX_R', 'FLUX_Z', 'FLUX_W1', 'FLUX_W2', 'FLUX_W3', 'FLUX_W4', \n",
    "            #'FLUX_IVAR_U', 'FLUX_IVAR_I', 'FLUX_IVAR_Y',             \n",
    "            'FLUX_IVAR_G', 'FLUX_IVAR_R', 'FLUX_IVAR_Z', 'FLUX_IVAR_W1', 'FLUX_IVAR_W2', 'FLUX_IVAR_W3', 'FLUX_IVAR_W4', \n",
    "            #'MW_TRANSMISSION_U', 'MW_TRANSMISSION_I', 'MW_TRANSMISSION_Y', \n",
    "            'MW_TRANSMISSION_G', 'MW_TRANSMISSION_R', 'MW_TRANSMISSION_Z', \n",
    "            'MW_TRANSMISSION_W1', 'MW_TRANSMISSION_W2', 'MW_TRANSMISSION_W3', 'MW_TRANSMISSION_W4', \n",
    "            #'NOBS_U', 'NOBS_I', 'NOBS_Y',\n",
    "            'NOBS_G', 'NOBS_R', 'NOBS_Z', 'NOBS_W1', 'NOBS_W2', 'NOBS_W3', 'NOBS_W4',\n",
    "            #'RCHISQ_U', 'RCHISQ_G', 'RCHISQ_R', 'RCHISQ_I', 'RCHISQ_Z', 'RCHISQ_Y',\n",
    "            #'RCHISQ_W1', 'RCHISQ_W2', 'RCHISQ_W3', 'RCHISQ_W4', \n",
    "            #'FRACFLUX_U', 'FRACFLUX_I', 'FRACFLUX_Y', \n",
    "            'FRACFLUX_G', 'FRACFLUX_R', 'FRACFLUX_Z', 'FRACFLUX_W1', 'FRACFLUX_W2', 'FRACFLUX_W3', 'FRACFLUX_W4', \n",
    "            'FRACMASKED_G', 'FRACMASKED_R', 'FRACMASKED_Z', #'FRACMASKED_U', 'FRACMASKED_I', 'FRACMASKED_Y', \n",
    "            'FRACIN_G', 'FRACIN_R', 'FRACIN_Z', #'FRACIN_U', 'FRACIN_I', 'FRACIN_Y', \n",
    "            'ANYMASK_G', 'ANYMASK_R', 'ANYMASK_Z', #'ANYMASK_U', 'ANYMASK_I', 'ANYMASK_Y', \n",
    "            'ALLMASK_G', 'ALLMASK_R', 'ALLMASK_Z', 'WISEMASK_W1', 'WISEMASK_W2', #'ALLMASK_U', 'ALLMASK_I', 'ALLMASK_Y', \n",
    "            'PSFSIZE_G', 'PSFSIZE_R', 'PSFSIZE_Z', #'PSFSIZE_U', 'PSFSIZE_I', 'PSFSIZE_Y',\n",
    "            'PSFDEPTH_G', 'PSFDEPTH_R', 'PSFDEPTH_Z', #'PSFDEPTH_U', 'PSFDEPTH_I', 'PSFDEPTH_Y', \n",
    "            'GALDEPTH_G', 'GALDEPTH_R', 'GALDEPTH_Z', #'GALDEPTH_U', 'GALDEPTH_I', 'GALDEPTH_Y', \n",
    "            'WISE_COADD_ID', 'FRACDEV', 'FRACDEV_IVAR', \n",
    "            'SHAPEDEV_R', 'SHAPEDEV_R_IVAR', 'SHAPEDEV_E1', 'SHAPEDEV_E1_IVAR', 'SHAPEDEV_E2', 'SHAPEDEV_E2_IVAR',\n",
    "            'SHAPEEXP_R', 'SHAPEEXP_R_IVAR', 'SHAPEEXP_E1', 'SHAPEEXP_E1_IVAR', 'SHAPEEXP_E2', 'SHAPEEXP_E2_IVAR']\n",
    "    \n",
    "    lsdr = 'dr9-north'\n",
    "    lsfile = os.path.join( os.getenv('REDMAPPER_DIR'), rmversion, \n",
    "                          'legacysurvey-{}-{}-{}-lgt5.fits'.format(lsdr, galtype, rmversion) )\n",
    "    dr9north = Table(fitsio.read(lsfile, ext=1, upper=True, rows=index, columns=cols))\n",
    "    print('Read {} galaxies from {}'.format(len(dr9north), lsfile))\n",
    "\n",
    "    lsdr = 'dr9-south'\n",
    "    lsfile = os.path.join( os.getenv('REDMAPPER_DIR'), rmversion, \n",
    "                           'legacysurvey-{}-{}-{}-lgt5.fits'.format(lsdr, galtype, rmversion) )\n",
    "    ls = Table(fitsio.read(lsfile, ext=1, upper=True, rows=index, columns=dr9north.colnames))\n",
    "    print('Read {} galaxies from {}'.format(len(ls), lsfile))\n",
    "\n",
    "    # If both DR8-north and DR8-south, decide based on grz depth.\n",
    "    both = (ls['RELEASE'] != 0) * (dr9north['RELEASE'] != 0)\n",
    "    if np.sum(both) > 0:\n",
    "        print('  Found {} galaxies with both north+south photometry.'.format(np.sum(both)))\n",
    "        pdb.set_trace()\n",
    "        usedr9north = ( (dr9north['PSFDEPTH_G'][both] > ls['PSFDEPTH_G'][both]) * \n",
    "                        (dr9north['PSFDEPTH_R'][both] > ls['PSFDEPTH_R'][both]) * \n",
    "                        (dr9north['PSFDEPTH_Z'][both] > ls['PSFDEPTH_Z'][both]) )\n",
    "        if np.sum(usedr6) > 0:\n",
    "            print('  Using deeper DR6 photometry for {}/{} galaxies.'.format(\n",
    "                np.sum(usedr6), np.sum(both)))\n",
    "            ls[both][usedr6] = dr6[both][usedr6]\n",
    "            \n",
    "    # If no DR7, use DR6.\n",
    "    usedr6 = (ls['RELEASE'] == 0) * (dr6['RELEASE'] != 0)\n",
    "    if np.sum(usedr6) > 0:\n",
    "        print('  Using DR6 for {} galaxies without DR7 photometry.'.format(np.sum(usedr6)))\n",
    "        ls[usedr6] = dr6[usedr6]\n",
    "\n",
    "    # Next, we have to deal with the fact that the the redmapper catalog contains \n",
    "    # duplicates (via 'ID').  Consequently, the coordinate-matching code only \n",
    "    # matched to *one* of the members, but the rest of the code in this notebook \n",
    "    # needs all the entries populated (because although they have the same `ID`, they \n",
    "    # have different `MEM_MATCH_ID` values, i.e., they belong to different clusters).\n",
    "    \n",
    "    # For example, consider ID 23136319, which appears on rows 4161 and\n",
    "    # 4632.  In the legacyhalos catalog only one entry is populated, e.g.,\n",
    "\n",
    "    # RELEASE BRICKID BRICKNAME ... SHAPEEXP_E1_IVAR SHAPEEXP_E2 SHAPEEXP_E2_IVAR\n",
    "    # int32   int32    bytes8  ...     float32        float32       float32\n",
    "    # ------- ------- --------- ... ---------------- ----------- ----------------\n",
    "    # 7000  498662  1402p305 ...              0.0         0.0              0.0\n",
    "    #    0       0           ...              0.0         0.0              0.0\n",
    "\n",
    "    # even though these are the same object.\n",
    "\n",
    "    # The script below (written by Chun-Hao To) finds all the duplicates in the \n",
    "    # redmapper catalog (via 'ID'), find the entry in the legacyhalos catalog that \n",
    "    # is populated (e.g., with RELEASE != 0) and then copies over the data to\n",
    "    # the entries that are empty.    \n",
    "    \n",
    "    if satellites:\n",
    "        print('Processing duplicates in the satellites catalog.')\n",
    "        t0 = time.time()\n",
    "        \n",
    "        redm_pd = pd.DataFrame({'ID': satid.byteswap().newbyteorder()})\n",
    "        #redm_pd = pd.DataFrame.from_records(satid)\n",
    "        redm_pd['index'] = pd.Series(np.arange(len(satid)), index=redm_pd.index)\n",
    "\n",
    "        # Find duplicates\n",
    "        duplicatedmask = redm_pd.duplicated(subset=['ID'], keep=False)\n",
    "        redm_pd_duplicated = redm_pd[duplicatedmask]\n",
    "        \n",
    "        group = redm_pd_duplicated.groupby(['ID'])\n",
    "        \n",
    "        for name, grp in group:\n",
    "            entry = None\n",
    "            for index in grp['index']:\n",
    "                temp = ls[index]\n",
    "                if temp['RELEASE'] != 0:\n",
    "                    entry = temp\n",
    "            if entry is not None:\n",
    "                for index in grp['index']:\n",
    "                    ls[index] = entry\n",
    "            #pdb.set_trace()\n",
    "                \n",
    "            # Old (slow!) way to accomplish the same result using numpy.\n",
    "            if False:\n",
    "                # https://stackoverflow.com/questions/30003068/get-a-list-of-all-indices-of-repeated-elements-in-a-numpy-array\n",
    "                usatid, inverse, count = np.unique(satid, return_inverse=True, return_counts=True)        \n",
    "        \n",
    "                dupindx = np.where(count > 1)[0]\n",
    "                if len(dupindx) > 0:\n",
    "                    rows, cols = np.where(inverse == dupindx[:, np.newaxis])\n",
    "                    _, inverse_rows = np.unique(rows, return_index=True)\n",
    "                    duplist = np.split(cols, inverse_rows[1:])\n",
    "                    print('  Time: {:.3f} min'.format( (time.time() - t0)/60 ))\n",
    "\n",
    "                    for cat, label in zip( (ls, dr6), ('DR7', 'DR6') ):\n",
    "                        print('  Processing {} duplicates for {}:'.format(len(duplist), label))\n",
    "                        t0 = time.time()\n",
    "                        for indx in duplist:\n",
    "                            this = cat['RELEASE'][indx] != 0\n",
    "                            if (np.sum(this) > 0) & (np.sum(this) < len(indx)):\n",
    "                                cat[indx[~this]] = cat[indx[this]]\n",
    "                        print('    Time: {} {:.3f} min'.format(label, (time.time() - t0)/60 ))\n",
    "\n",
    "        print('    Time: {:.3f} min'.format((time.time() - t0)/60 ))\n",
    "                \n",
    "    #print('  Found {} galaxies with DR7 photometry.'.format(np.sum(ls['RELEASE'] != 0)))\n",
    "    \n",
    "    miss = ls['RELEASE'] == 0\n",
    "    print('A total of {}/{} galaxies ({:.2f}%) have neither DR6 nor DR7 photometry.'.format(\n",
    "        np.sum(miss), len(ls), 100*np.sum(miss)/len(ls)))\n",
    "    \n",
    "    return ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = np.arange(1000)\n",
    "%time lssatall = read_legacysurvey(rmversion=rmversion, satellites=True, satid=rmsatall['ID'].data[index], index=index) # rmsat=rmsatall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ii = [[4161, 4632]]\n",
    "#lssatall[[4161, 4632]]\n",
    "#rmsatall[ii]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_redmapper(rmversion='v6.3.1', index=None, satellites=False):\n",
    "    \"\"\"Read the parent redMaPPer cluster catalog and updated photometry.\n",
    "    \n",
    "    \"\"\"\n",
    "    if satellites:\n",
    "        suffix1, suffix2 = '_members', '-members'\n",
    "    else:\n",
    "        suffix1, suffix2 = '', '-centrals'\n",
    "    rmfile = os.path.join( os.getenv('REDMAPPER_DIR'), rmversion, \n",
    "                          'dr8_run_redmapper_{}_lgt5_catalog{}.fit'.format(rmversion, suffix1) )\n",
    "    rmphotfile = os.path.join( os.getenv('REDMAPPER_DIR'), rmversion, \n",
    "                          'redmapper-{}-lgt5{}-sdssWISEphot-{}.fits'.format(rmversion, suffix2, sdssdr) )\n",
    "    \n",
    "    rm = Table(fitsio.read(rmfile, ext=1, upper=True, rows=index))\n",
    "    rmphot = Table(fitsio.read(rmphotfile, ext=1, upper=True, rows=index))\n",
    "\n",
    "    print('Read {} galaxies from {}'.format(len(rm), rmfile))\n",
    "    print('Read {} galaxies from {}'.format(len(rmphot), rmphotfile))\n",
    "    \n",
    "    rm.rename_column('RA', 'RA_REDMAPPER')\n",
    "    rm.rename_column('DEC', 'DEC_REDMAPPER')\n",
    "    rmphot.rename_column('RA', 'RA_SDSS')\n",
    "    rmphot.rename_column('DEC', 'DEC_SDSS')\n",
    "    rmphot.rename_column('OBJID', 'SDSS_OBJID')\n",
    "\n",
    "    assert(np.sum(rmphot['MEM_MATCH_ID'] - rm['MEM_MATCH_ID']) == 0)\n",
    "    if satellites:\n",
    "        assert(np.sum(rmphot['ID'] - rm['ID']) == 0)\n",
    "        rm.remove_columns( ('ID', 'MEM_MATCH_ID') )\n",
    "    else:\n",
    "        rm.remove_column('MEM_MATCH_ID')\n",
    "    rmout = hstack( (rmphot, rm) )\n",
    "    del rmphot, rm\n",
    "\n",
    "    # Add a central_id column\n",
    "    #rmout.rename_column('MEM_MATCH_ID', 'CENTRAL_ID')\n",
    "    #cid = ['{:07d}'.format(cid) for cid in rmout['MEM_MATCH_ID']]\n",
    "    #rmout.add_column(Column(name='CENTRAL_ID', data=cid, dtype='U7'), index=0)\n",
    "    \n",
    "    return rmout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Centrals\n",
    "Require a match with redMaPPer and non-zero depth in all three bands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_legacysurvey(lscat, rmcat):\n",
    "    good = np.where(\n",
    "        (lscat['GALDEPTH_G'] > 0) * \n",
    "        (lscat['GALDEPTH_R'] > 0) * \n",
    "        (lscat['GALDEPTH_Z'] > 0) * \n",
    "        (lscat['NOBS_G'] > 1) * \n",
    "        (lscat['NOBS_R'] > 1) * \n",
    "        (lscat['NOBS_Z'] > 1) *\n",
    "        (np.sum(rmcat['MODELMAGGIES'] == 0, axis=1) != 5) )[0] # missing SDSS photometry\n",
    "    return good"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmcenall = read_redmapper(rmversion=rmversion, index=index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lscenall = read_legacysurvey(rmversion=rmversion, index=index)\n",
    "assert(len(rmcenall) == len(lscenall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cenmatched = select_legacysurvey(lscenall, rmcenall)\n",
    "print('Identified {} / {} ({:.2f}%) centrals with grz photometry (nobs>1) and a match to redMaPPer.'.format(\n",
    "    len(cenmatched), len(lscenall), 100*len(cenmatched)/len(lscenall)))\n",
    "lscen = lscenall[cenmatched]\n",
    "lscen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmcen = rmcenall[cenmatched]\n",
    "rmcen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.scatter( lscen['RA'], (lscen['RA'] - rmcen['RA_SDSS']) * 3600, s=1)\n",
    "#plt.scatter( lscen['DEC'], (lscen['DEC'] - rmcen['DEC_SDSS']) * 3600, s=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.scatter(rmcenall['RA_SDSS'], rmcenall['DEC_SDSS'], s=1, label='redMaPPer/v6.3.1')\n",
    "ax.scatter(rmcen['RA_SDSS'], rmcen['DEC_SDSS'], s=1, alpha=0.1, \n",
    "           marker='.', label='DR6/DR7 Matched')\n",
    "ax.set_xlabel('RA')\n",
    "ax.set_ylabel('Dec')\n",
    "ax.set_ylim(-20, 80)\n",
    "ax.invert_xaxis()\n",
    "lgnd = ax.legend(loc='upper left', frameon=False, fontsize=10, ncol=2)\n",
    "for ll in lgnd.legendHandles:\n",
    "    ll._sizes = [30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unpack the candidate central galaxies from the satellites / members catalog.\n",
    "We are not analyzing the full set of satellites."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time rmsatall = read_redmapper(rmversion=rmversion, satellites=True, index=index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if index is None:\n",
    "    satid = rmsatall['ID'].data\n",
    "else:\n",
    "    satid = rmsatall['ID'].data[index]\n",
    "%time lssatall = read_legacysurvey(rmversion=rmversion, satellites=True, satid=satid, index=index)\n",
    "assert(len(rmsatall) == len(lssatall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_central_candidates(cen, sat, ls):\n",
    "    \"\"\"Create a hash table connecting, for each cluster, all the candidate \n",
    "    centrals' ID numbers to an index in the satellites catalog.  The clever \n",
    "    algorithm used here is by Chun-Hao To (Stanford).\n",
    "    \n",
    "    \"\"\"    \n",
    "    ncen, ncand = cen['ID_CENT'].shape\n",
    "\n",
    "    #offset = sat['ID'].min()\n",
    "    #g_index = dok_matrix( (np.max(sat['ID']) - offset + 1, 1), dtype=np.int )\n",
    "    #g_index[sat['ID'] - offset] = np.array( range( len(sat) ) )[:, np.newaxis]\n",
    "    \n",
    "    # Create a DataFrame for the catalog of centrals.\n",
    "    cen_temp = [cen['ID_CENT'][:, ii] for ii in range(ncand)]\n",
    "    cen_temp.append(cen['MEM_MATCH_ID'])\n",
    "    columns = ['ID_CENT_{}'.format(ii) for ii in range(ncand)]\n",
    "    columns.append('MEM_MATCH_ID_CEN')\n",
    "               \n",
    "    cen_pd = pd.DataFrame.from_records(np.array(cen_temp).T, columns=columns)\n",
    "    del cen_temp, columns\n",
    "\n",
    "    # Create DataFrame for the satellites / members.\n",
    "    sat_pd = pd.DataFrame.from_records(sat[['ID', 'MEM_MATCH_ID']].as_array())\n",
    "    sat_pd['index'] = pd.Series(np.arange(len(sat)), index=sat_pd.index)\n",
    "\n",
    "    # Create the mapping between them\n",
    "    cengalindex = np.zeros_like(cen['ID_CENT'])\n",
    "    pcen = np.zeros( len(sat) ).astype('f4')\n",
    "    primary_central = np.zeros( len(sat) ).astype(bool)\n",
    "    \n",
    "    for ii in range(ncand):\n",
    "        # Old algorithm which doesn't deal with duplicates correctly.\n",
    "        #index = np.where( cen['ID_CENT'][:, ii] - offset >= 0 )[0]\n",
    "        #cengalindex[index, ii] = g_index[cen['ID_CENT'][index, ii] - offset]\n",
    "        merged = pd.merge(cen_pd, sat_pd, left_on=['ID_CENT_{}'.format(ii), 'MEM_MATCH_ID_CEN'], \n",
    "                          right_on=['ID', 'MEM_MATCH_ID'], suffixes=('_original','_matched'))\n",
    "        cengalindex[:, ii] = merged['index']\n",
    "        pcen[cengalindex[:, ii]] = cen['P_CEN'][:, ii]\n",
    "        if ii == 0:\n",
    "            primary_central[cengalindex[:, ii]] = True\n",
    "        \n",
    "    cengalindex = cengalindex.flatten()\n",
    "        \n",
    "    candcen = sat[cengalindex]\n",
    "    candcen.add_column(Column(name='P_CEN', data=pcen[cengalindex]), index=1)\n",
    "    candcen.add_column(Column(name='PRIMARY_CENTRAL', data=primary_central[cengalindex]), index=2)\n",
    "\n",
    "    return candcen, ls[cengalindex]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time rmcandcenall, lscandcenall = get_central_candidates(rmcenall, rmsatall, lssatall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candcenmatched = select_legacysurvey(lscandcenall, rmcandcenall)\n",
    "print('Identified {} / {} ({:.2f}%) candidate centrals with grz photometry and a match to redMaPPer.'.format(\n",
    "    len(candcenmatched), len(lscandcenall), 100*len(candcenmatched)/len(lscandcenall)))\n",
    "lscandcen = lscandcenall[candcenmatched]\n",
    "#lscandcen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmcandcen = rmcandcenall[candcenmatched]\n",
    "rmcandcen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional sample cuts\n",
    "\n",
    "Exclude the sources with the shallowest DR6/DR7 *grz* photometry based on the estimated point-source depth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "band = ['G', 'R', 'Z']\n",
    "targdepth = [24.0, 23.4, 22.5] # target 5-sigma depth\n",
    "meddepth, P10depth = np.zeros((3)), np.zeros((3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dd = 22.5-2.5*np.log10(5/np.sqrt(lscen['PSFDEPTH_Z']))\n",
    "#nobs = lscen['NOBS_G']\n",
    "#_ = plt.hist(dd, bins=100)\n",
    "#print(dd.min(), np.sum(dd < 22), np.sum(nobs <= 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color = ('blue', 'green', 'red')\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4), sharey=True)\n",
    "for ii, (tt, bb, col) in enumerate( zip( targdepth, band, color ) ):\n",
    "    cendepth = 22.5 - 2.5 * np.log10( 5 / np.sqrt(lscen['PSFDEPTH_{}'.format(bb)]) )\n",
    "    candcendepth = 22.5 - 2.5 * np.log10( 5 / np.sqrt(lscandcen['PSFDEPTH_{}'.format(bb)]) )\n",
    "    \n",
    "    meddepth[ii] = np.percentile(cendepth, [50])\n",
    "    P10depth[ii] = np.percentile(cendepth, [10])\n",
    "    print('{} depth: P10: {:.3f}, median = {:.3f}, target = {:.3f}'.format(\n",
    "        bb.lower(), P10depth[ii], meddepth[ii], tt))\n",
    "    \n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter('ignore')\n",
    "        # centrals\n",
    "        nn, bins, patches = ax1.hist(cendepth, bins=100, histtype='step', cumulative=True,\n",
    "                                     label=bb.lower(), normed=True, color=col, lw=2)\n",
    "        patches[0].set_xy(patches[0].get_xy()[:-1]) # delete the last point\n",
    "        # candidate centrals\n",
    "        nn, bins, patches = ax2.hist(candcendepth, bins=100, histtype='step', cumulative=True,\n",
    "                                     label=bb.lower(), normed=True, color=col, lw=2)\n",
    "        patches[0].set_xy(patches[0].get_xy()[:-1]) # delete the last point\n",
    "    \n",
    "    #ax.axvline(x=tt, ls='--', color=col, lw=2, alpha=1.0)\n",
    "    #ax.axvline(x=meddepth[ii], ls='-', color=col, lw=1, alpha=0.9)\n",
    "    for ax in (ax1, ax2):\n",
    "        ax.axvline(x=P10depth[ii], ls='-', color=col, lw=1, alpha=0.9)\n",
    "\n",
    "ax1.legend(loc='upper left')\n",
    "ax1.set_ylabel('Fraction of Sample')\n",
    "ax1.set_title('Primary Centrals')\n",
    "ax2.set_title('Candidate Centrals')\n",
    "for ax in (ax1, ax2):\n",
    "    ax.set_xlabel('Imaging Depth (5$\\sigma$, AB mag)')\n",
    "    ax.set_xlim(21, 26)\n",
    "    \n",
    "fig.subplots_adjust(hspace=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "depthcut = (23.5, 23.0, 22.0)\n",
    "cendepthcut = np.ones(len(lscen)).astype(bool)\n",
    "candcendepthcut = np.ones(len(lscandcen)).astype(bool)\n",
    "for ii, bb in enumerate(['G', 'R', 'Z']):\n",
    "    cendepth = 22.5 - 2.5 * np.log10( 5 / np.sqrt(lscen['PSFDEPTH_{}'.format(bb)]) )\n",
    "    candcendepth = 22.5 - 2.5 * np.log10( 5 / np.sqrt(lscandcen['PSFDEPTH_{}'.format(bb)]) )\n",
    "    cendepthcut *= cendepth > depthcut[ii]\n",
    "    candcendepthcut *= candcendepth > depthcut[ii]\n",
    "    #cendepthcut *= cendepth > p10depth[ii]\n",
    "    #satdepthcut *= satdepth > p10depth[ii]\n",
    "print('{} / {} ({:.2f}%) centrals pass the depth cuts in all three bands.'.format(\n",
    "    np.sum(cendepthcut), len(lscen), 100*np.sum(cendepthcut)/len(lscen)))\n",
    "print('{} / {} ({:.2f}%) candidate centrals pass the depth cuts in all three bands.'.format(\n",
    "    np.sum(candcendepthcut), len(lscandcen), 100*np.sum(candcendepthcut)/len(lscandcen)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cut and join the redMaPPer (central & satellite) and LS catalogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmcendeep = rmcen[cendepthcut]\n",
    "lscendeep = lscen[cendepthcut]\n",
    "cen = hstack( (rmcendeep, lscendeep) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "candcen = hstack( (rmcandcen[candcendepthcut], lscandcen[candcendepthcut]) )\n",
    "#del rmcandcen, lscandcen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _normhist(xx, ax, label=None, alpha=1.0, lw=2, bins=100):\n",
    "    _, _, _ = ax.hist(xx, weights=np.ones_like(xx) / float(len(xx)), bins=bins, \n",
    "                      histtype='step', label=label, lw=lw, alpha=alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4), sharey=True)\n",
    "\n",
    "_normhist(rmcenall['Z'], ax1, label='All Centrals', lw=3)\n",
    "_normhist(rmcen['Z'], ax1, label='Matching Centrals', alpha=0.8)\n",
    "_normhist(rmcendeep['Z'], ax1, label='After Depth Cuts', alpha=0.8)\n",
    "ax1.set_xlabel('Redshift')\n",
    "ax1.set_ylabel('Fraction of Centrals')\n",
    "ax1.legend(loc='upper left')\n",
    "\n",
    "_normhist(np.log10(rmcenall['LAMBDA_CHISQ']), ax2, lw=3)\n",
    "_normhist(np.log10(rmcen['LAMBDA_CHISQ']), ax2, alpha=0.8)\n",
    "_normhist(np.log10(rmcendeep['LAMBDA_CHISQ']), ax2, alpha=0.8)\n",
    "ax2.set_xlabel('$\\log_{10}$ (Cluster Richness $\\lambda$)')\n",
    "#ax2.set_ylabel('Fraction of Galaxies')\n",
    "fig.subplots_adjust(wspace=0.03)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the total area subtended by the final sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_area(nside=256, qaplot=True):\n",
    "    \"\"\"Get the unique area of the sample.\"\"\"\n",
    "    \n",
    "    areaperpix = hp.nside2pixarea(nside, degrees=True)\n",
    "    samplepix = radec2pix(nside, cen['RA'].data, cen['DEC'].data)\n",
    "    print('Subdividing the sample into nside={} healpixels with area={:.4f} deg2 per pixel.'.format(\n",
    "        nside, areaperpix))\n",
    "\n",
    "    outpixmap = []\n",
    "    for dr, release in zip( ('dr6.0', 'dr7.1'), (6000, 7000) ):\n",
    "        # Read the pixel weight map which quantifies the imaging footprint\n",
    "        pixfile = os.path.join( legacyhalos.io.sample_dir(), 'pixweight-{}-0.22.0.fits'.format(dr) )\n",
    "        pixmap = Table(fitsio.read(pixfile))\n",
    "        pixmap['DR'] = dr.upper()\n",
    "    \n",
    "        these = cen['RELEASE'].data == release\n",
    "        thesepix = np.unique(samplepix[these])\n",
    "    \n",
    "        # Only keep non-empty healpixels.\n",
    "        keep = ( (pixmap['FRACAREA'][thesepix] > 0) * \n",
    "                (pixmap['PSFDEPTH_G'][thesepix] > 0) * # p10depth[0]) * \n",
    "                (pixmap['PSFDEPTH_R'][thesepix] > 0) * # p10depth[1]) * \n",
    "                (pixmap['PSFDEPTH_Z'][thesepix] > 0)   # p10depth[2]) \n",
    "               )\n",
    "        outpixmap.append(pixmap[thesepix][keep])\n",
    "    outpixmap = vstack(outpixmap)\n",
    "    \n",
    "    if False:\n",
    "        print('Clamping FRACAREA at unity!')\n",
    "        toobig = outpixmap['FRACAREA'] > 1\n",
    "        if np.sum(toobig) > 0:\n",
    "            outpixmap['FRACAREA'][toobig] = 1.0\n",
    "\n",
    "    # Don't double-count area, where DR6 and DR7 overlap.\n",
    "    _, keep = np.unique(outpixmap['HPXPIXEL'], return_index=True)\n",
    "    dup = np.delete( np.arange(len(outpixmap)), keep )\n",
    "    \n",
    "    # Code to double-check for duplicates and to ensure every object \n",
    "    # has been assigned a healpixel.\n",
    "    # for pp in outpixmap['HPXPIXEL'][keep]:\n",
    "    #     if np.sum( pp == outpixmap['HPXPIXEL'][keep] ) > 1:\n",
    "    #         print('Duplicate!')\n",
    "    #         import pdb ; pdb.set_trace()\n",
    "    #     if np.sum( pp == samplepix ) == 0:\n",
    "    #         print('Missing healpixel!')\n",
    "    #         import pdb ; pdb.set_trace()\n",
    "    \n",
    "    area = np.sum(outpixmap['FRACAREA'][keep]) * areaperpix\n",
    "    duparea = np.sum(outpixmap['FRACAREA'][dup]) * areaperpix\n",
    "\n",
    "    if qaplot:\n",
    "        uu = np.in1d(samplepix, outpixmap['HPXPIXEL'][keep])\n",
    "        dd = np.in1d(samplepix, outpixmap['HPXPIXEL'][dup])\n",
    "        fig, ax = plt.subplots()\n",
    "        ax.scatter(cen['RA'][uu], cen['DEC'][uu], s=1, marker='s',\n",
    "                   label=r'Unique: {:.1f} deg$^{{2}}$'.format(area))\n",
    "        ax.scatter(cen['RA'][dd], cen['DEC'][dd], s=1, marker='s',\n",
    "                   label=r'Overlapping: {:.1f} deg$^{{2}}$'.format(duparea))\n",
    "        ax.set_xlim(0, 360)\n",
    "        ax.set_ylim(-15, 80)\n",
    "        #ax.legend(loc='upper right', fontsize=12, frameon=False)\n",
    "        ax.invert_xaxis()\n",
    "        lgnd = ax.legend(loc='upper left', frameon=False, fontsize=10, ncol=2)\n",
    "        for ll in lgnd.legendHandles:\n",
    "            ll._sizes = [30]        \n",
    "        \n",
    "    return area, duparea, outpixmap[keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "area, duparea, pixmap = get_area()\n",
    "print('Unique area = {:.3f} deg2\\nOverlapping area = {:.3f} deg2'.format(area, duparea))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pixmap), len(np.unique(pixmap['HPXPIXEL']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create jackknife samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jackknife_samples(pixmap, nside_pixmap=256, nside_jack=4):\n",
    "    \"\"\"Split the sample into ~equal area chunks and write out a table.\n",
    "    \n",
    "    \"\"\"\n",
    "    from astropy.io import fits\n",
    "    \n",
    "    area_jack = hp.nside2pixarea(nside_jack, degrees=True)\n",
    "    area_pixmap = hp.nside2pixarea(nside_pixmap, degrees=True)\n",
    "    print('Jackknife nside = {} with area = {:.3f} deg2'.format(nside_jack, area_jack))\n",
    "    \n",
    "    pix_jack = radec2pix(nside_jack, cen['RA'].data, cen['DEC'].data)\n",
    "    pix_pixmap = radec2pix(nside_pixmap, cen['RA'].data, cen['DEC'].data)\n",
    "    \n",
    "    upix_jack = np.unique(pix_jack)\n",
    "    upix_jack = upix_jack[np.argsort(upix_jack)]\n",
    "    npix = len(upix_jack)\n",
    "    \n",
    "    ra_jack, dec_jack = pix2radec(nside_jack, upix_jack)\n",
    "    \n",
    "    out = Table()\n",
    "    out['HPXPIXEL'] = upix_jack\n",
    "    out['RA'] = ra_jack\n",
    "    out['DEC'] = dec_jack\n",
    "    out['AREA'] = np.zeros(npix).astype('f4')\n",
    "    out['NCEN'] = np.zeros(npix).astype('int')\n",
    "    \n",
    "    for ii, pp in enumerate(upix_jack):\n",
    "        these = np.where( pp == pix_jack )[0]\n",
    "        indx = np.where( np.in1d( pixmap['HPXPIXEL'].data, pix_pixmap[these] ) )[0]\n",
    "        uindx = np.unique(indx)\n",
    "        #print(pp, len(indx), len(uindx))\n",
    "\n",
    "        out['AREA'][ii] = np.sum(pixmap['FRACAREA'][indx].data) * area_pixmap\n",
    "        out['NCEN'][ii] = len(these)\n",
    "        \n",
    "        #if ii == 4:\n",
    "        #    rbig, dbig = pix2radec(nside_jack, pp)\n",
    "        #    rsmall, dsmall = pix2radec(nside_pixmap, pixmap['HPXPIXEL'][indx].data)\n",
    "        #    rgal, dgal = sample['RA'][these], sample['DEC'][these]\n",
    "        #    plt.scatter(rgal, dgal, s=3, marker='o', color='green')\n",
    "        #    plt.scatter(rsmall, dsmall, s=3, marker='s', color='blue')\n",
    "        #    plt.scatter(rbig, dbig, s=75, marker='x', color='k')\n",
    "        #    plt.show()\n",
    "        #    import pdb ; pdb.set_trace() \n",
    "        \n",
    "    print('Writing {}'.format(jackfile))\n",
    "    hx = fits.HDUList()\n",
    "    hdu = fits.convenience.table_to_hdu(out)\n",
    "    hdu.header['EXTNAME'] = 'JACKKNIFE'\n",
    "    hdu.header['NSIDE'] = nside_jack\n",
    "    hx.append(hdu)\n",
    "    hx.writeto(jackfile, overwrite=True)\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nside_jack = 4\n",
    "jack = jackknife_samples(pixmap, nside_jack=nside_jack)\n",
    "njack = len(jack)\n",
    "jack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Check: total area = {:.3f}, total number of galaxies = {}'.format(\n",
    "    np.sum(jack['AREA']), np.sum(jack['NCEN'])))\n",
    "print('Mean / median area per pixel = {:.3f} / {:.3f} deg2'.format(\n",
    "    np.mean(jack['AREA']), np.median(jack['AREA'])))\n",
    "print('Mean / median number of centrals per pixel = {:.0f} / {:.0f}'.format(\n",
    "    np.mean(jack['NCEN']), np.median(jack['NCEN'])))\n",
    "_ = plt.hist(jack['AREA'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize the jackknife samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jackpix = legacyhalos.misc.radec2pix(nside_jack, cen['RA'].data, cen['DEC'].data)\n",
    "jack_ra, jack_dec = legacyhalos.misc.pix2radec(nside_jack, jack['HPXPIXEL'])\n",
    "fig, ax = plt.subplots()\n",
    "for ii in range(njack):\n",
    "    indx = np.where( jack['HPXPIXEL'][ii] == jackpix )[0]\n",
    "    ax.scatter(cen['RA'][indx], cen['DEC'][indx], s=1)\n",
    "    ax.text(jack_ra[ii], jack_dec[ii], '{:02d}'.format(ii), \n",
    "            va='center', ha='center')\n",
    "    #ax.text(jack_ra[ii], jack_dec[ii], '{:02d}'.format(jack['HPXPIXEL'][ii]))    \n",
    "ax.set_xlabel('RA')\n",
    "ax.set_ylabel('Dec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write out the final samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Writing {}'.format(cenfile))\n",
    "cen.write(cenfile, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Writing {}'.format(candcenfile))\n",
    "candcen.write(candcenfile, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#bb = lscandcenall[rmcandcenall['ID'] == 25404292]\n",
    "#bb[['GALDEPTH_G', 'GALDEPTH_R', 'GALDEPTH_Z', 'NOBS_G', 'NOBS_R', 'NOBS_Z']]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "docker: legacyhalos",
   "language": "python",
   "name": "legacyhalos"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
