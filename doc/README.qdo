I like the "qdo" utility by Stephen Bailey
https://bitbucket.org/berkeleylab/qdo/pull-requests/60?t=1

The idea is that you keep your list of jobs in a database, and then you launch
workers who query the database for work to do, and then they record their status
when they finish.

You can either keep full command-lines in the database, or you can keep whatever
value changes between jobs (eg, for you, you might want to keep the brick
names), and use the "--script" option to run a script that will get the brick
name as a command-line argument.

For example, you could create a queue like this,

> tablist /project/projectdirs/cosmo/data/legacysurvey/dr3/survey-bricks-dr3.fits.gz"[col brickname]" | awk '{print $2}' | tail -n +3 | qdo load mybricks -

and then you can look at the queue like

> qdo tasks mybricks | head
State       Task
Pending     0001m002
Pending     0001m005
Pending     0001m007
Pending     0001m010
...

Then you can create a script like demo.sh

#! /bin/bash
brick=$1
echo "I'm going to run $brick"
python mycode.py $brick > run-$brick.log 2>&1

You can test it out by running a single worker from the command line,

> chmod 755 ./demo.sh
> qdo do mybricks --script ./demo.sh

And you can launch jobs using the "shared" Slurm queue with a command like:

> qdo launch mybricks 1 --cores_per_worker=1 --keep_env --batchqueue shared --walltime 4:00:00 --script ./demo.sh --batchopts "-a 0-15"

where the "-a" launches an "array job", which is like N copies (here N=16) of a
job.

The wall-time you ask for isn't crucial -- if qdo finishes all the jobs in its
queue, it will just quit, and you won't get charged for time you don't use.  But
if you ask for a lot of wall time, your job can take a long time to be
scheduled, so it's easiest to make a guess of how much time you need and, say,
double it.

Nice features of qdo are that you don't need millions of slurm scripts, and it
keeps track of the status of your jobs so it's easy to re-run failed jobs and
stuff like that.  But it is overhead to learn and figure out...
